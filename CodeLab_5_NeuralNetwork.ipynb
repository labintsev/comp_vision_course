{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/balezz/cv_course_fa_mag/blob/main/CodeLab_3_NeuralNetwork.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHTx7qtNqTmr"
      },
      "source": [
        "# CodeLab 3 - Многослойные нейронные сети\n",
        "\n",
        "В этом задании вам предстоит:\n",
        "- реализовать слои ReLU и FullyConnected;\n",
        "- составить из них нейронную сеть;\n",
        "- переобучить сеть на небольшом наборе данных\n",
        "- обучить модель на полной выборке и получить точность > 40%\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4o0LkWMusLbT"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeojDZw6lGAf"
      },
      "source": [
        "## Загрузка датасета CIFAR-10 и нормализация данных"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Vksu9_hshtH"
      },
      "outputs": [],
      "source": [
        "# Очистим значения переменных, чтобы избежать проблем с излишним потреблением памяти\n",
        "try:\n",
        "   del X_train, y_train\n",
        "   del X_test, y_test\n",
        "   print('Clear previously loaded data.')\n",
        "except:\n",
        "   pass\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "\n",
        "# Проверим размер входных и выходных векторов.\n",
        "print('Training data shape: ', X_train.shape)\n",
        "print('Training labels shape: ', y_train.shape)\n",
        "print('Test data shape: ', X_test.shape)\n",
        "print('Test labels shape: ', y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbbr8Lsgshpf"
      },
      "outputs": [],
      "source": [
        "# Перед началом работы полезно посмотреть на данные.\n",
        "# Отобразим пример из каждого класса.\n",
        "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "num_classes = len(classes)\n",
        "samples_per_class = 7\n",
        "for y, cls in enumerate(classes):\n",
        "    idxs = np.flatnonzero(y_train == y)\n",
        "    idxs = np.random.choice(idxs, samples_per_class, replace=False)\n",
        "    for i, idx in enumerate(idxs):\n",
        "        plt_idx = i * num_classes + y + 1\n",
        "        plt.subplot(samples_per_class, num_classes, plt_idx)\n",
        "        plt.imshow(X_train[idx].astype('uint8'))\n",
        "        plt.axis('off')\n",
        "        if i == 0:\n",
        "            plt.title(cls)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "553jnmLFlrUU"
      },
      "outputs": [],
      "source": [
        "# Для удобства преобразуем двумерные изображения в одномерные вектора fp64\n",
        "X_train = np.reshape(X_train, (X_train.shape[0], -1)).astype(np.float64)\n",
        "X_test = np.reshape(X_test, (X_test.shape[0], -1)).astype(np.float64)\n",
        "y_train = y_train.flatten()\n",
        "y_test = y_test.flatten()\n",
        "# Проверим размер полученных данных\n",
        "print('Training data shape: ', X_train.shape)\n",
        "print('Test data shape: ', X_test.shape)\n",
        "print('Training label shape: ', y_train.shape)\n",
        "print('Test label shape: ', y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wV4zBk0-s3Y0"
      },
      "outputs": [],
      "source": [
        "# Разделим данные на выборки train, val, test\n",
        "\n",
        "num_training = 49000\n",
        "num_validation = 1000\n",
        "num_test = 1000\n",
        "num_dev = 16\n",
        "\n",
        "# Для валидации используем подвыборку train\n",
        "mask = range(num_training, num_training + num_validation)\n",
        "X_val = X_train[mask]\n",
        "y_val = y_train[mask]\n",
        "\n",
        "# На остальных данных из train будем тренировать модель\n",
        "mask = range(num_training)\n",
        "X_train = X_train[mask]\n",
        "y_train = y_train[mask]\n",
        "\n",
        "# Чтобы ускорить разработку, создадим также небольшую dev выборку \n",
        "mask = np.random.choice(num_training, num_dev, replace=False)\n",
        "X_dev = X_train[mask]\n",
        "y_dev = y_train[mask]\n",
        "\n",
        "# Для тестирования используем оригинальную выборку test\n",
        "mask = range(num_test)\n",
        "X_test = X_test[mask]\n",
        "y_test = y_test[mask]\n",
        "\n",
        "print('Train data shape: ', X_train.shape)\n",
        "print('Train labels shape: ', y_train.shape)\n",
        "print('Validation data shape: ', X_val.shape)\n",
        "print('Validation labels shape: ', y_val.shape)\n",
        "print('Test data shape: ', X_test.shape)\n",
        "print('Test labels shape: ', y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E11G6K95swaT"
      },
      "outputs": [],
      "source": [
        "# Нормализуем значения яркости пикселей \n",
        "mean_image = np.mean(X_train, axis=0)\n",
        "print(mean_image[:10]) \n",
        "\n",
        "# визуализируем среднюю яркость\n",
        "plt.figure(figsize=(4,4))\n",
        "plt.imshow(mean_image.reshape((32,32,3)).astype('uint8')) \n",
        "plt.show()\n",
        "\n",
        "# Вычтем средние значения яркости\n",
        "X_train -= mean_image\n",
        "X_val -= mean_image\n",
        "X_test -= mean_image\n",
        "X_dev -= mean_image\n",
        "\n",
        "\n",
        "print(X_train.shape, X_val.shape, X_test.shape, X_dev.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Id80AvrtmkUh"
      },
      "source": [
        "## Функции для сравнения аналитического градиента с градиентом, вычисленного методом конечных разностей. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZYwEb83qTm2"
      },
      "outputs": [],
      "source": [
        "def check_gradient(f, x, delta=1e-5, tol=1e-4):\n",
        "    \"\"\"\n",
        "    Checks the implementation of analytical gradient by comparing\n",
        "    it to numerical gradient using two-point formula\n",
        "    Arguments:\n",
        "      f: function that receives x and computes value and gradient\n",
        "      x: np array, initial point where gradient is checked\n",
        "      delta: step to compute numerical gradient\n",
        "      tol: tolerance for comparing numerical and analytical gradient\n",
        "    Return:\n",
        "      bool indicating whether gradients match or not\n",
        "    \"\"\"\n",
        "    assert isinstance(x, np.ndarray)\n",
        "    assert x.dtype == np.float\n",
        "    \n",
        "    orig_x = x.copy()\n",
        "    fx, analytic_grad = f(x)\n",
        "    assert np.all(np.isclose(orig_x, x, tol)), \"Functions shouldn't modify input variables\"\n",
        "\n",
        "    assert analytic_grad.shape == x.shape\n",
        "    analytic_grad = analytic_grad.copy()\n",
        "    \n",
        "    # We will go through every dimension of x and compute numeric\n",
        "    # derivative for it\n",
        "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
        "    while not it.finished:\n",
        "        ix = it.multi_index\n",
        "        analytic_grad_at_ix = analytic_grad[ix] \n",
        "        x0 = x.copy()\n",
        "        x1 = x.copy()\n",
        "        x0[ix] += delta\n",
        "        x1[ix] -= delta\n",
        "        \n",
        "        numeric_grad_at_ix = (f(x0)[0] - f(x1)[0]) / (2 * delta)\n",
        "        print(analytic_grad_at_ix , numeric_grad_at_ix)\n",
        "\n",
        "         #TODO compute value of numeric gradient of f to idx\n",
        "        if not np.isclose(numeric_grad_at_ix, analytic_grad_at_ix, tol):\n",
        "            print(\"Gradients are different at %s. Analytic: %2.5f, Numeric: %2.5f\" % (ix, analytic_grad_at_ix, numeric_grad_at_ix))\n",
        "            return False\n",
        "\n",
        "        it.iternext()\n",
        "    \n",
        "    print(\"Gradient check passed!\")\n",
        "    return True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hzjqjblan4aT"
      },
      "outputs": [],
      "source": [
        "def check_layer_gradient(layer, x, delta=1e-5, tol=1e-4):\n",
        "    \"\"\"\n",
        "    Checks gradient correctness for the input and output of a layer\n",
        "    Arguments:\n",
        "      layer: neural network layer, with forward and backward functions\n",
        "      x: starting point for layer input\n",
        "      delta: step to compute numerical gradient\n",
        "      tol: tolerance for comparing numerical and analytical gradient\n",
        "    Returns:\n",
        "      bool indicating whether gradients match or not\n",
        "    \"\"\"\n",
        "    output = layer.forward(x)\n",
        "    output_weight = np.random.randn(*output.shape)\n",
        "\n",
        "    def helper_func(x):\n",
        "        output = layer.forward(x)\n",
        "        loss = np.sum(output * output_weight)\n",
        "        d_out = np.ones_like(output) * output_weight\n",
        "        grad = layer.backward(d_out)\n",
        "        return loss, grad\n",
        "\n",
        "    return check_gradient(helper_func, x, delta, tol)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJZ0st4Fn7yB"
      },
      "outputs": [],
      "source": [
        "def check_layer_param_gradient(layer, x,\n",
        "                               param_name,\n",
        "                               delta=1e-5, tol=1e-4):\n",
        "    \"\"\"\n",
        "    Checks gradient correctness for the parameter of the layer\n",
        "    Arguments:\n",
        "      layer: neural network layer, with forward and backward functions\n",
        "      x: starting point for layer input\n",
        "      param_name: name of the parameter\n",
        "      delta: step to compute numerical gradient\n",
        "      tol: tolerance for comparing numerical and analytical gradient\n",
        "    Returns:\n",
        "      bool indicating whether gradients match or not\n",
        "    \"\"\"\n",
        "    param = layer.params()[param_name]\n",
        "    initial_w = param.value\n",
        "\n",
        "    output = layer.forward(x)\n",
        "    output_weight = np.random.randn(*output.shape)\n",
        "\n",
        "    def helper_func(w):\n",
        "        param.value = w\n",
        "        output = layer.forward(x)\n",
        "        loss = np.sum(output * output_weight)\n",
        "        d_out = np.ones_like(output) * output_weight\n",
        "        layer.backward(d_out)\n",
        "        grad = param.grad\n",
        "        return loss, grad\n",
        "\n",
        "    return check_gradient(helper_func, initial_w, delta, tol)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8YzP4cHcoFii"
      },
      "outputs": [],
      "source": [
        "def check_model_gradient(model, X, y,\n",
        "                         delta=1e-5, tol=1e-4):\n",
        "    \"\"\"\n",
        "    Checks gradient correctness for all model parameters\n",
        "    Arguments:\n",
        "      model: neural network model with compute_loss_and_gradients\n",
        "      X: batch of input data\n",
        "      y: batch of labels\n",
        "      delta: step to compute numerical gradient\n",
        "      tol: tolerance for comparing numerical and analytical gradient\n",
        "    Returns:\n",
        "      bool indicating whether gradients match or not\n",
        "    \"\"\"\n",
        "    params = model.params()\n",
        "\n",
        "    for param_key in params:\n",
        "        print(\"Checking gradient for %s\" % param_key)\n",
        "        param = params[param_key]\n",
        "        initial_w = param.value\n",
        "\n",
        "        def helper_func(w):\n",
        "            param.value = w\n",
        "            loss = model.compute_loss_and_gradients(X, y)\n",
        "            grad = param.grad\n",
        "            return loss, grad\n",
        "\n",
        "        if not check_gradient(helper_func, initial_w, delta, tol):\n",
        "            return False\n",
        "\n",
        "    return True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofWTpnBXnqMj"
      },
      "source": [
        "## Задание 1. ReLU Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RENeyXUWqTm8"
      },
      "source": [
        "Мы будем реализовывать необходимые нам слои по очереди. Каждый слой должен реализовать:\n",
        "- прямой проход (forward pass), который генерирует выход слоя по входу и запоминает необходимые данные\n",
        "- обратный проход (backward pass), который получает градиент по выходу слоя и вычисляет градиент по входу и по параметрам\n",
        "\n",
        "Начнем с ReLU, у которого обучаемых параметров нет."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mgCITVZ_vqqG"
      },
      "outputs": [],
      "source": [
        "class ReLULayer:\n",
        "    def __init__(self):\n",
        "        self.mask = None\n",
        "\n",
        "    def forward(self, X):\n",
        "        # TODO: Implement forward pass\n",
        "        # Hint: you'll need to save some information about X\n",
        "        # to use it later in the backward pass\n",
        "        #raise Exception(\"Not implemented!\")\n",
        "        pass\n",
        "\n",
        "    def backward(self, d_out):\n",
        "        \"\"\"\n",
        "        Backward pass\n",
        "        Arguments:\n",
        "        d_out, np array (batch_size, num_features) - gradient\n",
        "           of loss function with respect to output\n",
        "        Returns:\n",
        "        d_result: np array (batch_size, num_features) - gradient\n",
        "          with respect to input\n",
        "        \"\"\"\n",
        "        # TODO: Implement backward pass\n",
        "        # Your final implementation shouldn't have any loops\n",
        "        # raise Exception(\"Not implemented!\")\n",
        "        \n",
        "\n",
        "    def params(self):\n",
        "        # ReLU Doesn't have any parameters\n",
        "        return {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oc5nbuadqTm-"
      },
      "outputs": [],
      "source": [
        "# TODO: Implement ReLULayer \n",
        "# Note: you'll need to copy implementation of the gradient_check function from the previous assignment\n",
        "\n",
        "X = np.array([[1,-2,3],\n",
        "              [-1, 2, 0.1]\n",
        "              ])\n",
        "\n",
        "assert check_layer_gradient(ReLULayer(), X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IINMnKmovhE"
      },
      "source": [
        "## Задание 2. FullyConnected (Dense) Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFORmUG0qTm-"
      },
      "source": [
        "А теперь реализуем полносвязный слой (fully connected layer), у которого будет два массива параметров: W (weights) и B (bias).\n",
        "\n",
        "Все параметры наши слои будут использовать для параметров специальный класс `Param`, в котором будут храниться значения параметров и градиенты этих параметров, вычисляемые во время обратного прохода.\n",
        "\n",
        "Это даст возможность аккумулировать (суммировать) градиенты из разных частей функции потерь, например, из cross-entropy loss и regularization loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LwKPVms4wooe"
      },
      "outputs": [],
      "source": [
        "class Param:\n",
        "    \"\"\"\n",
        "    Trainable parameter of the model\n",
        "    Captures both parameter value and the gradient\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, value):\n",
        "        self.value = value\n",
        "        self.grad = np.zeros_like(value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "toNth1PKwbos"
      },
      "outputs": [],
      "source": [
        "class FullyConnectedLayer:\n",
        "    def __init__(self, n_input, n_output):\n",
        "        self.W = Param(0.001 * np.random.randn(n_input, n_output))\n",
        "        self.B = Param(0.001 * np.random.randn(1, n_output))\n",
        "        self.X = None\n",
        "\n",
        "    def forward(self, X):\n",
        "        # TODO: Implement forward pass\n",
        "        # Your final implementation shouldn't have any loops\n",
        "        # raise Exception(\"Not implemented!\")\n",
        "        pass\n",
        "\n",
        "\n",
        "    def backward(self, d_out):\n",
        "        \"\"\"\n",
        "        Backward pass\n",
        "        Computes gradient with respect to input and\n",
        "        accumulates gradients within self.W and self.B\n",
        "        Arguments:\n",
        "        d_out, np array (batch_size, n_output) - gradient\n",
        "           of loss function with respect to output\n",
        "        Returns:\n",
        "        d_result: np array (batch_size, n_input) - gradient\n",
        "          with respect to input\n",
        "        \"\"\"\n",
        "        # TODO: Implement backward pass\n",
        "        # Compute both gradient with respect to input\n",
        "        # and gradients with respect to W and B\n",
        "        # Add gradients of W and B to their `grad` attribute\n",
        "\n",
        "        # It should be pretty similar to linear classifier from\n",
        "        # the previous assignment\n",
        "        # raise Exception(\"Not implemented!\")\n",
        "        # print('d_out shape is ', d_out.shape)\n",
        "        # print('self.W shape is ', self.W.value.shape)\n",
        "        \n",
        "\n",
        "        return d_input\n",
        "\n",
        "    def params(self):\n",
        "        return {'W': self.W, 'B': self.B}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kI8wlgoCqTm_",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# TODO: Implement FullyConnected layer forward and backward methods\n",
        "assert check_layer_gradient(FullyConnectedLayer(3, 4), X)\n",
        "# TODO: Implement storing gradients for W and B\n",
        "assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'W')\n",
        "assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'B')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PJW7H6pqTnA"
      },
      "source": [
        "## Задание 3. Создаем двухслойную нейронную сеть\n",
        "\n",
        "Теперь мы реализуем простейшую нейронную сеть с двумя полносвязным слоями и нелинейностью ReLU. Реализуйте функцию `compute_loss_and_gradients`, она должна запустить прямой и обратный проход через оба слоя для вычисления градиентов.\n",
        "\n",
        "Не забудьте реализовать очистку градиентов в начале функции."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_UAbBDaB9I2"
      },
      "outputs": [],
      "source": [
        "# Copied from previous\n",
        "def softmax_with_cross_entropy(X, y):\n",
        "    \"\"\"\n",
        "    Computes softmax and cross-entropy loss for model predictions,\n",
        "    including the gradient\n",
        "    Arguments:\n",
        "      predictions, np array, shape is either (N) or (batch_size, N) -\n",
        "        classifier output\n",
        "      target_index: np array of int, shape is (1) or (batch_size) -\n",
        "        index of the true class for given sample(s)\n",
        "    Returns:\n",
        "      loss, single value - cross-entropy loss\n",
        "      dprediction, np array same shape as predictions - gradient of predictions by loss value\n",
        "    \"\"\"\n",
        "    N = X.shape[0]\n",
        "    X -= np.max(X)\n",
        "    Exps = np.exp(X)\n",
        "    # S.shape = N, C\n",
        "    S = Exps / Exps.sum(axis=1, keepdims=True)\n",
        "    # loss.shape = 1\n",
        "    loss = - np.log(S[range(N), y]).mean()\n",
        "    S[range(N), y] -= 1\n",
        "    d_out = S / N\n",
        "    return loss, d_out\n",
        "\n",
        "def l2_regularization(W, reg_strength):\n",
        "    loss = 0.5 * reg_strength * np.sum(W*W)\n",
        "    grad = np.dot(W,reg_strength)\n",
        "    return loss, grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iJi2DaKFw3ey"
      },
      "outputs": [],
      "source": [
        "class TwoLayerNet:\n",
        "    \"\"\" Neural network with two fully connected layers \"\"\"\n",
        "\n",
        "    def __init__(self, n_input, n_output, hidden_layer_size, reg):\n",
        "        \"\"\"\n",
        "        Initializes the neural network\n",
        "        Arguments:\n",
        "        n_input, int - dimension of the model input\n",
        "        n_output, int - number of classes to predict\n",
        "        hidden_layer_size, int - number of neurons in the hidden layer\n",
        "        reg, float - L2 regularization strength\n",
        "        \"\"\"\n",
        "        self.reg = reg\n",
        "        # TODO Create list with necessary layers\n",
        "        raise Exception(\"Not implemented!\")\n",
        "        \n",
        "\n",
        "\n",
        "    def compute_loss_and_gradients(self, X, y):\n",
        "        \"\"\"\n",
        "        Computes total loss and updates parameter gradients\n",
        "        on a batch of training examples\n",
        "        Arguments:\n",
        "        X, np array (batch_size, input_features) - input data\n",
        "        y, np array of int (batch_size) - classes\n",
        "        \"\"\"\n",
        "        \n",
        "        # Before running forward and backward pass through the model,\n",
        "        # clear parameter gradients aggregated from the previous pass\n",
        "        \n",
        "        # TODO Set parameter gradient to zeros\n",
        "        # Hint: using self.params() might be useful!\n",
        "        raise Exception(\"Not implemented!\")\n",
        "\n",
        "\n",
        "        # TODO Compute loss and fill param gradients\n",
        "        # by running forward and backward passes through the model\n",
        "\n",
        "        \n",
        "        # TODO After that, implement l2 regularization on all params\n",
        "        # Hint: self.params() is useful again!\n",
        "        # raise Exception(\"Not implemented!\")\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Produces classifier predictions on the set\n",
        "        Arguments:\n",
        "          X, np array (test_samples, num_features)\n",
        "        Returns:\n",
        "          y_pred, np.array of int (test_samples)\n",
        "        \"\"\"\n",
        "        # TODO: Implement predict\n",
        "        # Hint: some of the code of the compute_loss_and_gradients\n",
        "        # can be reused\n",
        "        raise Exception(\"Not implemented!\")\n",
        "        return pred\n",
        "\n",
        "    def params(self):\n",
        "        result = {}\n",
        "        name = 0\n",
        "        # TODO Implement aggregating all of the params\n",
        "\n",
        "        return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eLg4aUyAqTnA"
      },
      "outputs": [],
      "source": [
        "# TODO: implement compute_loss_and_gradients function\n",
        "model = TwoLayerNet(n_input = X_train.shape[1], n_output = 10, hidden_layer_size = 3, reg = 0)\n",
        "loss = model.compute_loss_and_gradients(X_train[:2], y_train[:2])\n",
        "\n",
        "# TODO Now implement backward pass and aggregate all of the params\n",
        "check_model_gradient(model, X_train[:2], y_train[:2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i17Dtu83qTnB"
      },
      "source": [
        "Теперь добавьте к модели регуляризацию - она должна прибавляться к loss и делать свой вклад в градиенты."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8dMNZMrPqTnC"
      },
      "outputs": [],
      "source": [
        "# TODO Now implement l2 regularization in the forward and backward pass\n",
        "model_with_reg = TwoLayerNet(n_input = X_train.shape[1], n_output = 10, hidden_layer_size = 3, reg = 1e1)\n",
        "loss_with_reg = model_with_reg.compute_loss_and_gradients(X_train[:2], y_train[:2])\n",
        "assert loss_with_reg > loss and not np.isclose(loss_with_reg, loss), \\\n",
        "    \"Loss with regularization (%2.4f) should be higher than without it (%2.4f)!\" % (loss, loss_with_reg)\n",
        "\n",
        "check_model_gradient(model_with_reg, X_train[:2], y_train[:2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OhlbDyIqTnD"
      },
      "source": [
        "Также реализуем функцию предсказания (вычисления значения) модели на новых данных.\n",
        "\n",
        "Какое значение точности мы ожидаем увидеть до начала тренировки?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kvgmwuEWyEuj"
      },
      "outputs": [],
      "source": [
        "def multiclass_accuracy(y_true, y_pred):\n",
        "  return np.mean(np.equal(y_true, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3BjF-QEgqTnD"
      },
      "outputs": [],
      "source": [
        "# Finally, implement predict function!\n",
        "\n",
        "# TODO: Implement predict function\n",
        "# What would be the value we expect?\n",
        "multiclass_accuracy(model_with_reg.predict(X_train[:30]), y_train[:30]) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRjzawUJqTnD"
      },
      "source": [
        "## Задание 4. Допишем код для процесса тренировки\n",
        "\n",
        "#### Уменьшение скорости обучения (learning rate decay)\n",
        "\n",
        "Одна из необходимых оптимизаций во время тренировки нейронных сетей - постепенное уменьшение скорости обучения по мере тренировки.\n",
        "\n",
        "Один из стандартных методов - уменьшение скорости обучения (learning rate) каждые N эпох на коэффициент d (часто называемый decay). Значения N и d, как всегда, являются гиперпараметрами и должны подбираться на основе эффективности на проверочных данных (validation data). \n",
        "\n",
        "В классе Trainer допишите код для обучения на батчах и уменьшения learning_rate decay.  \n",
        "\n",
        "Если все реализовано корректно, значение функции ошибки должно уменьшаться в некоторых эпохах. Не беспокойтесь пока про validation accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kYxUpX2Q5bTA"
      },
      "outputs": [],
      "source": [
        "from copy import deepcopy\n",
        "class Dataset:\n",
        "    \"\"\"\n",
        "    Utility class to hold training and validation data\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, train_X, train_y, val_X, val_y):\n",
        "        self.train_X = train_X\n",
        "        self.train_y = train_y\n",
        "        self.val_X = val_X\n",
        "        self.val_y = val_y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EXjIXATS3Tuc"
      },
      "outputs": [],
      "source": [
        "class Trainer:\n",
        "    \"\"\"\n",
        "    Trainer of the neural network models\n",
        "    Perform mini-batch SGD with the specified data, model,\n",
        "    training parameters and optimization rule\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model, dataset, optim,\n",
        "                 num_epochs=20,\n",
        "                 batch_size=20,\n",
        "                 learning_rate=1e-2,\n",
        "                 learning_rate_decay=1.0):\n",
        "        \"\"\"\n",
        "        Initializes the trainer\n",
        "        Arguments:\n",
        "        model - neural network model\n",
        "        dataset, instance of Dataset class - data to train on\n",
        "        optim - optimization method (see optim.py)\n",
        "        num_epochs, int - number of epochs to train\n",
        "        batch_size, int - batch size\n",
        "        learning_rate, float - initial learning rate\n",
        "        learning_rate_decal, float - ratio for decaying learning rate\n",
        "           every epoch\n",
        "        \"\"\"\n",
        "        self.dataset = dataset\n",
        "        self.model = model\n",
        "        self.optim = optim\n",
        "        self.batch_size = batch_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.num_epochs = num_epochs\n",
        "        self.learning_rate_decay = learning_rate_decay\n",
        "\n",
        "        self.optimizers = None\n",
        "\n",
        "    def setup_optimizers(self):\n",
        "        params = self.model.params()\n",
        "        self.optimizers = {}\n",
        "        for param_name, param in params.items():\n",
        "            self.optimizers[param_name] = deepcopy(self.optim)\n",
        "\n",
        "    def compute_accuracy(self, X, y):\n",
        "        \"\"\"\n",
        "        Computes accuracy on provided data using mini-batches\n",
        "        \"\"\"\n",
        "        indices = np.arange(X.shape[0])\n",
        "        sections = np.arange(self.batch_size, X.shape[0], self.batch_size)\n",
        "        batches_indices = np.array_split(indices, sections)\n",
        "\n",
        "        pred = np.zeros_like(y)\n",
        "\n",
        "        for batch_indices in batches_indices:\n",
        "            batch_X = X[batch_indices]\n",
        "            pred_batch = self.model.predict(batch_X)\n",
        "            pred[batch_indices] = pred_batch\n",
        "\n",
        "        return multiclass_accuracy(pred, y)\n",
        "\n",
        "    def fit(self):\n",
        "        \"\"\"\n",
        "        Trains a model\n",
        "        \"\"\"\n",
        "        if self.optimizers is None:\n",
        "            self.setup_optimizers()\n",
        "\n",
        "        num_train = self.dataset.train_X.shape[0]\n",
        "\n",
        "        loss_history = []\n",
        "        train_acc_history = []\n",
        "        val_acc_history = []\n",
        "        \n",
        "        for epoch in range(self.num_epochs):\n",
        "            shuffled_indices = np.arange(num_train)\n",
        "            np.random.shuffle(shuffled_indices)\n",
        "            sections = np.arange(self.batch_size, num_train, self.batch_size)\n",
        "            batches_indices = np.array_split(shuffled_indices, sections)\n",
        "\n",
        "            batch_losses = []\n",
        "\n",
        "            for batch_indices in batches_indices:\n",
        "                # TODO Generate batches based on batch_indices and\n",
        "                # use model to generate loss and gradients for all\n",
        "                # the params\n",
        "\n",
        "                raise Exception(\"Not implemented!\")\n",
        "\n",
        "\n",
        "            if np.not_equal(self.learning_rate_decay, 1.0):\n",
        "                # TODO: Implement learning rate decay\n",
        "                raise Exception(\"Not implemented!\")\n",
        "\n",
        "\n",
        "            ave_loss = np.mean(batch_losses)\n",
        "\n",
        "            train_accuracy = self.compute_accuracy(self.dataset.train_X,\n",
        "                                                   self.dataset.train_y)\n",
        "\n",
        "            val_accuracy = self.compute_accuracy(self.dataset.val_X,\n",
        "                                                 self.dataset.val_y)\n",
        "\n",
        "            print(\"Loss: %f, Train accuracy: %f, val accuracy: %f\" %\n",
        "                  (batch_losses[-1], train_accuracy, val_accuracy))\n",
        "\n",
        "            loss_history.append(ave_loss)\n",
        "            train_acc_history.append(train_accuracy)\n",
        "            val_acc_history.append(val_accuracy)\n",
        "\n",
        "        return loss_history, train_acc_history, val_acc_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hI7UwbYYHGQD"
      },
      "outputs": [],
      "source": [
        "class SGD:\n",
        "    \"\"\"\n",
        "    Implements vanilla SGD update\n",
        "    \"\"\"\n",
        "    def update(self, w, d_w, learning_rate):\n",
        "        \"\"\"\n",
        "        Performs SGD update\n",
        "        Arguments:\n",
        "        w, np array - weights\n",
        "        d_w, np array, same shape as w - gradient\n",
        "        learning_rate, float - learning rate\n",
        "        Returns:\n",
        "        updated_weights, np array same shape as w\n",
        "        \"\"\"\n",
        "        return w - d_w * learning_rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tu2GMEUxqTnE"
      },
      "outputs": [],
      "source": [
        "model = TwoLayerNet(n_input = X_train.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
        "dataset = Dataset(X_dev, y_dev, X_dev, y_dev)\n",
        "trainer = Trainer(model, dataset, SGD(), learning_rate = 1e-4)\n",
        "\n",
        "# TODO Implement missing pieces in Trainer.fit function\n",
        "# You should expect loss to go down every epoch, even if it's slow\n",
        "loss_history, train_history, val_history = trainer.fit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gdYzqWT7qTnG"
      },
      "outputs": [],
      "source": [
        "# TODO Implement learning rate decay inside Trainer.fit method\n",
        "# Decay should happen once per epoch\n",
        "\n",
        "model = TwoLayerNet(n_input = X_train.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
        "dataset = Dataset(X_dev, y_dev, X_val, y_val)\n",
        "trainer = Trainer(model, dataset, SGD(), learning_rate = 1e-4, learning_rate_decay=0.99)\n",
        "\n",
        "initial_learning_rate = trainer.learning_rate\n",
        "loss_history, train_history, val_history = trainer.fit()\n",
        "\n",
        "assert trainer.learning_rate < initial_learning_rate, \"Learning rate should've been reduced\"\n",
        "assert trainer.learning_rate > 0.5*initial_learning_rate, \"Learning rate shouldn'tve been reduced that much!\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_-F6WL1qTnI"
      },
      "source": [
        "## Задание 5. Переобучим модель (overfit) на маленьком наборе данных\n",
        "\n",
        "Хороший способ проверить, все ли реализовано корректно - переобучить сеть на маленьком наборе данных.  \n",
        "Наша модель обладает достаточной мощностью, чтобы приблизить маленький набор данных идеально, поэтому мы ожидаем, что на нем мы быстро дойдем до 100% точности на небольшом тренировочном наборе. \n",
        "\n",
        "Найдите гипепараметры, для которых процесс тренировки сходится быстрее.\n",
        "Если все реализовано корректно, то существуют параметры, при которых train accuracy = 1.0 в **20** эпох или еще быстрее.\n",
        "\n",
        "Если этого не происходит, то где-то была допущена ошибка!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ytzdNQcUqTnJ"
      },
      "outputs": [],
      "source": [
        "model = TwoLayerNet(n_input = X_train.shape[1], n_output = 10, hidden_layer_size = 64, reg = 1e-1)\n",
        "dataset = Dataset(X_dev, y_dev, X_val, y_val)\n",
        "trainer = Trainer(model, dataset, SGD(), learning_rate=1e-4, num_epochs=20, batch_size=8)\n",
        "\n",
        "# You should expect this to reach 1.0 training accuracy \n",
        "loss_history, train_history, val_history = trainer.fit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvbgvvqpqTnM"
      },
      "source": [
        "## Задание 6. Натренируйте лучшую нейросеть!\n",
        "\n",
        "Можно добавлять и изменять параметры, менять количество нейронов в слоях сети и как угодно экспериментировать. \n",
        "\n",
        "Добейтесь точности лучше **40%** на Test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3svNLCiuqTnN",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Let's train the best one-hidden-layer network we can\n",
        "\n",
        "learning_rate = 1e-4\n",
        "reg_strength = 1e-3\n",
        "learning_rate_decay = 0.999\n",
        "hidden_layer_size = 128\n",
        "num_epochs = 200\n",
        "batch_size = 64\n",
        "\n",
        "best_classifier = None\n",
        "best_val_accuracy = None\n",
        "\n",
        "loss_history = []\n",
        "train_history = []\n",
        "val_history = []\n",
        "\n",
        "# TODO find the best hyperparameters to train the network\n",
        "# Don't hesitate to add new values to the arrays above, perform experiments, use any tricks you want\n",
        "# You should expect to get to at least 40% of valudation accuracy\n",
        "# Save loss/train/history of the best classifier to the variables above\n",
        "\n",
        "print('best validation accuracy achieved: %f' % best_val_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1T6Xm6PxqTnN"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15, 7))\n",
        "plt.subplot(211)\n",
        "plt.title(\"Loss\")\n",
        "plt.plot(loss_history)\n",
        "plt.subplot(212)\n",
        "plt.title(\"Train/validation accuracy\")\n",
        "plt.plot(train_history)\n",
        "plt.plot(val_history)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "A3-NeuralNetwork.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
