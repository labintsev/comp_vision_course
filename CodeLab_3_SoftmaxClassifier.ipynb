{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/balezz/cv_course_fa_mag/blob/main/CodeLab_2_SoftmaxClassifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PZ9OL7QjDAuV",
    "tags": [
     "pdf-title"
    ]
   },
   "source": [
    "# Линейный классификатор Softmax \n",
    "\n",
    "Для выполнения этого задания нужно будет дописать код в этом ноутбуке  \n",
    "\n",
    "В этом упражнении Вам предстоит:\n",
    "\n",
    "- реализовать функцию потерь (**loss**) для Softmax классификатора\n",
    "- реализовать векторизованную функцию для вычисления **аналитического градиента**\n",
    "- **оптимизировать** матрицу весов W с помощью стохастического градиентного спуска **SGD**\n",
    "- найти лучшие гиперпараметры **learning rate и regularization** \n",
    "- **визуализировать** матрицу оптимальных весов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aZFniEy8DAuc",
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (16.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oI6Y5fHiJx9e"
   },
   "source": [
    "# Загрузка датасета CIFAR-10 и предварительная подготовка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "065Yd2AjDAud",
    "outputId": "b1d0c108-9d8d-42ef-c571-45f82a0542db",
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Проверим размер входных и выходных векторов.\n",
    "print('Training data shape: ', X_train.shape)\n",
    "print('Training labels shape: ', y_train.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 482
    },
    "id": "xLmw-REjJ8Do",
    "outputId": "723deedf-efaa-43e7-cf37-11d776b7a874"
   },
   "outputs": [],
   "source": [
    "# Перед началом работы полезно посмотреть на данные.\n",
    "# Отобразим пример из каждого класса.\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "num_classes = len(classes)\n",
    "samples_per_class = 7\n",
    "for y, cls in enumerate(classes):\n",
    "    idxs = np.flatnonzero(y_train == y)\n",
    "    idxs = np.random.choice(idxs, samples_per_class, replace=False)\n",
    "    for i, idx in enumerate(idxs):\n",
    "        plt_idx = i * num_classes + y + 1\n",
    "        plt.subplot(samples_per_class, num_classes, plt_idx)\n",
    "        plt.imshow(X_train[idx].astype('uint8'))\n",
    "        plt.axis('off')\n",
    "        if i == 0:\n",
    "            plt.title(cls)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l1MTg2q5KDZw",
    "outputId": "ffe1a50b-30c9-44dc-9eb9-17d21cbae39a"
   },
   "outputs": [],
   "source": [
    "# Для удобства преобразуем двумерные изображения в одномерные вектора fp64\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], -1)).astype(np.float64)\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], -1)).astype(np.float64)\n",
    "y_train = y_train.flatten()\n",
    "y_test = y_test.flatten()\n",
    "# Проверим размер полученных данных\n",
    "print('Training data shape: ', X_train.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Training label shape: ', y_train.shape)\n",
    "print('Test label shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 303
    },
    "id": "HShnobjq5cTl",
    "outputId": "e1b6ab79-24d2-4b64-b2d2-d65a48752da9"
   },
   "outputs": [],
   "source": [
    "# Нормализуем значения яркости пикселей \n",
    "mean_image = np.mean(X_train, axis=0)\n",
    "print(mean_image[:10]) \n",
    "\n",
    "# визуализируем среднюю яркость\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.imshow(mean_image.reshape((32,32,3)).astype('uint8')) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I5NPvm_fKG9o",
    "outputId": "9ce4e35c-2b73-4e7d-8eef-14d0432d506d"
   },
   "outputs": [],
   "source": [
    "# Вычтем средние значения яркости\n",
    "X_train -= mean_image\n",
    "X_test -= mean_image\n",
    "\n",
    "# Добавим к вектору исходных данных фиктивный признак с постоянным значением 1.\n",
    "# Этот трюк позволит избежать лишних вычислений: x @ W + b  => x' @ W'\n",
    "X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gOvIlepPDAug",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Softmax Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7ye3uzuW8Eqw",
    "outputId": "1554ec62-abf9-49e9-f731-837086a54883"
   },
   "outputs": [],
   "source": [
    "# Небольшая фишка с итерацией по заданному измерению ndarray\n",
    "p = np.arange(40).reshape(4, 10)\n",
    "y = 1, 2, 3, 4\n",
    "\n",
    "print('p = \\n', p)\n",
    "print('y = \\n', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g1ZlkAee8WkQ",
    "outputId": "3f492907-43a6-4c73-c628-9eb21d35518d"
   },
   "outputs": [],
   "source": [
    "# Возьмем первый элемент из нулевой строки, второй элемент из первой строки и так далее.\n",
    "p[range(4), y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tuning",
    "tags": [
     "code"
    ]
   },
   "outputs": [],
   "source": [
    "# Реализуйте эффективную векторизованную  \n",
    "# функцию вычисления loss и dW\n",
    "\n",
    "def softmax_loss(W, X, y, reg):\n",
    "    \"\"\"\n",
    "    Функция потерь Softmax, векторизованная версия.\n",
    "    Входы и выходы такие же, как у softmax_loss_naive.\n",
    "    \"\"\"\n",
    "    # Инициализируем значения loss и градиента нулями.\n",
    "    loss = 0.0\n",
    "    dW = np.zeros_like(W)\n",
    "\n",
    "    #############################################################################\n",
    "    # TODO: Вычислить softmax loss и его градиент без явных циклов.            #\n",
    "    # Сохраните значение функции потерь в переменной loss и градиент в dW.     #\n",
    "    # Будьте внимательны к численной нестабильности при экспонентах. Не забывайте#\n",
    "    # про регуляризацию!                                                      #\n",
    "    #############################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "    return loss, dW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hbd-SlbFDAuh",
    "outputId": "7c117cee-ca86-4e8b-eec6-f4ac9562ea74",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Инициализируем веса значениями близкими, но не равными нулю\n",
    "W = np.random.randn(3073, 10) * 0.0001\n",
    "loss, grad = softmax_loss(W, X_test, y_test, 0.0)\n",
    "\n",
    "# Обязательно проверим правильность реализации функции\n",
    "# Для 10 классов loss должен быть около -log(0.1).\n",
    "print('loss: %f' % loss)\n",
    "print('Начальное значение Loss = %f' % (-np.log(0.1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fa6KyB0yHnkk"
   },
   "source": [
    "**Вопрос 1**\n",
    "\n",
    "Объясните, почему мы предположили, что для 10 классов при весах близких к нулю\n",
    " softmax loss приблизительно равен -log(0.1)?  \n",
    "$\\color{blue}{\\textit Ответ:}$ *заполнить здесь* \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "94MroGpUK1_t"
   },
   "source": [
    "# Стохастический градиентный спуск"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4yF-MdGCK8lZ"
   },
   "outputs": [],
   "source": [
    "# Реализуйте SGD и проверьте результат \n",
    "\n",
    "class SoftmaxClassifier():\n",
    "    def __init__(self):\n",
    "        self.W = None\n",
    "\n",
    "    def train(self, X, y, learning_rate=1e-3, reg=1e-5, num_iters=100,\n",
    "              batch_size=32, verbose=True):\n",
    "        \"\"\"\n",
    "        Обучение классификатора с помощью стохастического градиентного спуска.\n",
    "        Входы:\n",
    "        - X: numpy массив формы (N, D) с обучающими данными; N примеров, каждое\n",
    "          размерности D.\n",
    "        - y: numpy массив формы (N,) с метками; y[i] = c означает, что X[i] имеет\n",
    "          метку 0 <= c < C для C классов.\n",
    "        - learning_rate: (float) скорость обучения.\n",
    "        - reg: (float) сила регуляризации.\n",
    "        - num_iters: (integer) число шагов оптимизации.\n",
    "        - batch_size: (integer) число примеров в мини-пакете.\n",
    "        - verbose: (boolean) если True, выводить прогресс оптимизации.\n",
    "        Возвращает:\n",
    "        Список значений функции потерь на каждой итерации обучения.\n",
    "        \"\"\"\n",
    "        num_train, dim = X.shape\n",
    "        num_classes = np.max(y) + 1 # предполагаем, что y принимает значения 0...K-1\n",
    "        if self.W is None:\n",
    "            # лениво инициализируем W\n",
    "            self.W = 0.001 * np.random.randn(dim, num_classes)\n",
    "\n",
    "        # Запускаем стохастический градиентный спуск для оптимизации W\n",
    "        loss_history = []\n",
    "        for it in range(num_iters):\n",
    "            X_batch = None\n",
    "            y_batch = None\n",
    "\n",
    "            #########################################################################\n",
    "            # TODO:                                                                 #\n",
    "            # Выберите batch_size элементов из обучающей выборки и соответствующие  #\n",
    "            # метки для использования в этом шаге градиентного спуска. Сохраните    #\n",
    "            # данные в X_batch и соответствующие метки в y_batch; после выборки     #\n",
    "            # X_batch должен иметь размерность (batch_size, dim),                   #\n",
    "            # а y_batch — (batch_size,)                                             #\n",
    "            #                                                                       #\n",
    "            # Подсказка: используйте np.random.choice для генерации индексов.       #\n",
    "            # Выбор с возвращением (replacement) быстрее, чем без него.             #\n",
    "            #########################################################################\n",
    "            # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "\n",
    "            # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "            # вычисляем loss и градиент\n",
    "            loss,  grad = self.loss(X_batch, y_batch, reg)\n",
    "            loss_history.append(loss)\n",
    "\n",
    "            # выполняем обновление параметров\n",
    "            #########################################################################\n",
    "            # TODO:                                                                 #\n",
    "            # Обновите веса, используя градиент и скорость обучения.                #\n",
    "            #########################################################################\n",
    "            # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "\n",
    "\n",
    "            # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "            if verbose and it % 100 == 0:\n",
    "                print('iteration %d / %d: loss %f '  % (it, num_iters, loss))\n",
    "\n",
    "        return loss_history\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Использует обученные веса этого линейного классификатора для предсказания\n",
    "        меток для входных данных.\n",
    "        Входы:\n",
    "        - X: numpy массив формы (N, D) с данными; N примеров размерности D.\n",
    "        Возвращает:\n",
    "        - y_pred: массив предсказанных меток длины N, каждое значение — целое число\n",
    "                  с предсказанным классом.\n",
    "        \"\"\"\n",
    "        y_pred = np.zeros(X.shape[0])\n",
    "        ###########################################################################\n",
    "        # TODO:                                                                   #\n",
    "        # Реализуйте этот метод. Сохраните предсказанные метки в y_pred.          #\n",
    "        ###########################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        return y_pred\n",
    "\n",
    "    def loss(self, X_batch, y_batch, reg):\n",
    "        \"\"\"\n",
    "        Вычисляет функцию потерь и её производную.\n",
    "        Входы:\n",
    "        - X_batch: numpy массив формы (N, D) с мини-пакетом из N точек; каждая точка размерности D.\n",
    "        - y_batch: numpy массив формы (N,) с метками для мини-пакета.\n",
    "        - reg: (float) сила регуляризации.\n",
    "        Возвращает: кортеж из:\n",
    "        - loss — одномерное число (float)\n",
    "        - gradient по отношению к self.W; массив той же формы, что и W\n",
    "        \"\"\"\n",
    "        return softmax_loss(self.W, X_batch, y_batch, reg)\n",
    "    \n",
    "    def save_weights(self, path):\n",
    "        np.save(path, self.W)\n",
    "    \n",
    "    def load_weights(self, path):\n",
    "        self.W = np.load(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RkYWqdb2K1St"
   },
   "outputs": [],
   "source": [
    "softmax_cls = SoftmaxClassifier()\n",
    "tic = time.time()\n",
    "loss_hist = softmax_cls.train(\n",
    "    X_train[:1000], y_train[:1000], \n",
    "    learning_rate=1e-4, reg=1e-3,\n",
    "    num_iters=1000, verbose=True)\n",
    "\n",
    "toc = time.time()\n",
    "print('That took %fs' % (toc - tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nIAQM6c-MkoH"
   },
   "outputs": [],
   "source": [
    "# Построим график зависимости loss от количества итераций\n",
    "plt.plot(loss_hist)\n",
    "plt.xlabel('Iteration number')\n",
    "plt.ylabel('Loss value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LxoJZ186Mvgv"
   },
   "outputs": [],
   "source": [
    "# оцените точность предсказания на выборках train и val\n",
    "y_train_pred = softmax_cls.predict(X_train)\n",
    "print('training accuracy: %f' % (np.mean(y_train == y_train_pred) ))\n",
    "y_test_pred = softmax_cls.predict(X_test)\n",
    "print('validation accuracy: %f' % (np.mean(y_test == y_test_pred) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NKwv4sTdNEvU"
   },
   "source": [
    "# Поиск лучших гиперпараметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZLB88C5BM-_D"
   },
   "outputs": [],
   "source": [
    "# Используйте валидационную выборку для выбора лучших гиперпараметров \n",
    "# (learning rate and regularization strength)\n",
    "# Добейтесь точности не меньше 0.38 на выборке test.\n",
    "# Используйте словарь results в котором \n",
    "# ключи - кортеж  (learning_rate, regularization_strength)\n",
    "# значения - (training_accuracy, validation_accuracy)\n",
    "# Точность вычисляется как отношение числа верно предсказанных классов \n",
    "# к объему выборки\n",
    "results = {}\n",
    "best_val = -1   \n",
    "best_softmax = None # Лучший экземпляр Softmax classifier \n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Напишите код, позволяющий найти лучшее значение гиперпараметров на val       #\n",
    "# выборке. Для каждой комбинации гиперпараметров обучите классификатор         #\n",
    "# на train выборке, вычислите точность на выборках train, val и сохраните      #\n",
    "# результат в словарь results. Лучшее значение точности сохраните в best_val   #\n",
    "# лучший классификатор - в best_softmax                                        #\n",
    "#                                                                              #\n",
    "################################################################################\n",
    "\n",
    "# Пример списка допустимых значений. Можете изменить на свое усмотрение.\n",
    "learning_rates = [1e-7, 5e-5]\n",
    "regularization_strengths = [1e-1, 1, 10]\n",
    "\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    \n",
    "# Вывод результатов.\n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print('lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
    "                lr, reg, train_accuracy, val_accuracy))\n",
    "    \n",
    "print('best validation accuracy achieved during cross-validation: %f' % best_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D9amiJoBNDIC"
   },
   "outputs": [],
   "source": [
    "# Визуализируем результаты кросс-валидации\n",
    "import math\n",
    "import pdb\n",
    "\n",
    "# pdb.set_trace()\n",
    "\n",
    "x_scatter = [math.log10(x[0]) for x in results]\n",
    "y_scatter = [math.log10(x[1]) for x in results]\n",
    "\n",
    "# график accuracy на обучении\n",
    "marker_size = 100\n",
    "colors = [results[x][0] for x in results]\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.tight_layout(pad=3)\n",
    "plt.scatter(x_scatter, y_scatter, marker_size, c=colors, cmap=plt.cm.coolwarm)\n",
    "plt.colorbar()\n",
    "plt.xlabel('log learning rate')\n",
    "plt.ylabel('log regularization strength')\n",
    "plt.title('CIFAR-10 training accuracy')\n",
    "\n",
    "# график accuracy на валидации\n",
    "colors = [results[x][1] for x in results] # default size of markers is 20\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.scatter(x_scatter, y_scatter, marker_size, c=colors, cmap=plt.cm.coolwarm)\n",
    "plt.colorbar()\n",
    "plt.xlabel('log learning rate')\n",
    "plt.ylabel('log regularization strength')\n",
    "plt.title('CIFAR-10 validation accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ghx3RGPPDAuk",
    "tags": [
     "pdf-inline"
    ]
   },
   "source": [
    "# Точность на test выборке\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IFONVgKYr5bS"
   },
   "outputs": [],
   "source": [
    "y_test_pred = best_softmax.predict(X_test)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print('softmax on raw pixels final test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_softmax.save_weights('softmax_weights.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ap4ZunISDAul"
   },
   "outputs": [],
   "source": [
    "# Визуализируем веса W для каждого класса\n",
    "w = best_softmax.W[:-1,:] # отбросим фиктивное измерение bias\n",
    "w = w.reshape(32, 32, 3, 10)\n",
    "\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "plt.figure(figsize=(16, 10))\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    \n",
    "    # Масштабируем веса в значения от 0 до 255 для визуализации\n",
    "    wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
    "    plt.imshow(wimg.astype('uint8'), interpolation='quadric')\n",
    "    plt.axis('off')\n",
    "    plt.title(classes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6odQCbWQDAum"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "CodeLab_2_SoftmaxClassifier.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
